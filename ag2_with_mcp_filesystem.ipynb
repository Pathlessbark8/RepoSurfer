{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90b04378-221f-45ce-a9cc-cb58467b919d",
   "metadata": {},
   "source": [
    "# MCP (3 Agent system to parse any local repository and explain the code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c40dc04",
   "metadata": {},
   "source": [
    "## Author: Dhruv Saxena (Cambridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16ed7aaa-e6c9-42c4-bad6-b5c16176ae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.sse import sse_client\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "from autogen import LLMConfig\n",
    "from autogen.agentchat import AssistantAgent\n",
    "from autogen.mcp import create_toolkit\n",
    "import json\n",
    "import anyio\n",
    "import asyncio\n",
    "\n",
    "\n",
    "\n",
    "# Only needed for Jupyter notebooks\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from autogen.agentchat.group import (\n",
    "    AgentNameTarget,\n",
    "    AgentTarget,\n",
    "    AskUserTarget,\n",
    "    ContextExpression,\n",
    "    ContextStr,\n",
    "    ContextStrLLMCondition,\n",
    "    ContextVariables,\n",
    "    ExpressionAvailableCondition,\n",
    "    ExpressionContextCondition,\n",
    "    GroupChatConfig,\n",
    "    GroupChatTarget,\n",
    "    Handoffs,\n",
    "    NestedChatTarget,\n",
    "    OnCondition,\n",
    "    OnContextCondition,\n",
    "    ReplyResult,\n",
    "    RevertToUserTarget,\n",
    "    SpeakerSelectionResult,\n",
    "    StayTarget,\n",
    "    StringAvailableCondition,\n",
    "    StringContextCondition,\n",
    "    StringLLMCondition,\n",
    "    TerminateTarget,\n",
    ")\n",
    "\n",
    "from autogen import (\n",
    "    ConversableAgent,\n",
    "    GroupChat,\n",
    "    GroupChatManager,\n",
    "    LLMConfig,\n",
    "    UserProxyAgent,\n",
    "    register_function,\n",
    ")\n",
    "\n",
    "from autogen.agentchat.group.patterns import (\n",
    "    DefaultPattern,\n",
    "    ManualPattern,\n",
    "    AutoPattern,\n",
    "    RandomPattern,\n",
    "    RoundRobinPattern,\n",
    ")\n",
    "\n",
    "\n",
    "from autogen import ConversableAgent, UpdateSystemMessage\n",
    "from autogen.agents.experimental import DocAgent\n",
    "import os\n",
    "import copy\n",
    "from typing import Any, Dict, List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "from autogen.agentchat import initiate_group_chat, a_initiate_group_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "522f8e10-d797-42f1-a3c5-ff0473f82573",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp_server_path = Path(\"mcp_filesystem.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d5d303-7256-4507-8c2f-6eff59e4085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "joker_message = \"\"\"\n",
    "You are a code exploration assistant.\n",
    "\n",
    "Your tasks:\n",
    "1. Recursively list all files using the `list_all_files_recursive` tool.\n",
    "2. Read source files (.py, .js, .cpp, .md, etc.) using the `read_file` tool only when needed.\n",
    "3. Answer user questions about how the code works, starting from high-level and drilling into details only when necessary.\n",
    "4. Identify the entry point and key modules, summarize file purposes, and show how components interact.\n",
    "5. Think step-by-step — you can ask yourself questions and call tools accordingly before replying.\n",
    "6. After completing your summary, ask the user what they would like to explore next.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pydantic import BaseModel,Field\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "default_llm_config = {'cache_seed': 42,\n",
    "                     'temperature': 1.,\n",
    "                     'top_p': 0.05,\n",
    "                     'config_list': [{'model': 'gpt-4o',\n",
    "                                      'api_key': \"\", #ADD YOUR API KEY\n",
    "                                      'api_type': 'openai'}],\n",
    "                     'timeout': 1200}\n",
    "\n",
    "joker_config_list = copy.deepcopy(default_llm_config)\n",
    "# joker_config_list['config_list'][0]['response_format'] = JokeResponse\n",
    "\n",
    "\n",
    "joker =  ConversableAgent(\n",
    "    name=\"joker\",\n",
    "    system_message=r\"\"\"\n",
    "        Your job is to summarize the codebase in your folder recursively:\n",
    "        1. Use `list_all_files_recursive` to get all code files.\n",
    "        2. Use `read_file` on each relevant file (e.g., .py, .js, .cpp).\n",
    "        3. Summarize the purpose of each file.\n",
    "        4. Identify how they connect and find the entry point.\n",
    "        5. After you're done summarizing, say \"The file has been read.\"\n",
    "\n",
    "        After the summary, ask the user if they want to:\n",
    "        - Drill down into a specific file\n",
    "        - Understand a function or module\n",
    "        - Or ask another question about the codebase.\n",
    "        \"\"\",\n",
    "    llm_config = joker_config_list,\n",
    "    update_agent_state_before_reply=[UpdateSystemMessage(joker_message),],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "task = \"\"\"\n",
    "Recursively traverse the user given directory and summarize what the code inside the directory does.\n",
    "\"\"\"\n",
    "\n",
    "initial_agent = joker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04998ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from autogen import UserProxyAgent\n",
    "\n",
    "class AsyncUserProxyAgent(UserProxyAgent):\n",
    "    async def a_get_human_input(self, prompt: str) -> str:\n",
    "        print(prompt)\n",
    "        loop = asyncio.get_event_loop()\n",
    "        return await loop.run_in_executor(None, input, \"> \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baf8175-01ce-48bf-b9f3-1f6bfcaa0b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def create_toolkit_and_run(session: ClientSession) -> None:\n",
    "    # Create toolkit with tools\n",
    "    toolkit = await create_toolkit(session=session)\n",
    "\n",
    "    # Define agents\n",
    "    mcp_agent = ConversableAgent(\n",
    "        name=\"mcp_agent\",\n",
    "        system_message=r\"Read the file in your folder and skip non-code files.\",\n",
    "        llm_config=default_llm_config,\n",
    "    )\n",
    "\n",
    "    joker = ConversableAgent(\n",
    "        name=\"joker\",\n",
    "        system_message=r\"You are Joker. Help the user understand the summarized codebase.\",\n",
    "        llm_config=default_llm_config,\n",
    "    )\n",
    "\n",
    "    user_proxy = AsyncUserProxyAgent(name=\"real_user\")\n",
    "\n",
    "\n",
    "    # Register tools with MCP\n",
    "    toolkit.register_for_llm(mcp_agent)\n",
    "    toolkit.register_for_execution(mcp_agent)\n",
    "\n",
    "    # Handoff chain: mcp → joker → user → joker → user ...\n",
    "    mcp_agent.handoffs.set_after_work(AgentTarget(joker))\n",
    "    joker.handoffs.set_after_work(AgentTarget(user_proxy))\n",
    "    user_proxy.handoffs.set_after_work(AgentTarget(joker))\n",
    "\n",
    "\n",
    "    # Optional LLM condition to reinforce flow\n",
    "    mcp_agent.handoffs.add_llm_conditions([\n",
    "        OnCondition(\n",
    "            condition=StringLLMCondition(prompt=\"The files have been read.\"),\n",
    "            target=AgentTarget(joker),\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "    # Ask user for directory path\n",
    "    user_input = input(\"Enter directory path to traverse and summarize: \")\n",
    "\n",
    "    # Define task prompt\n",
    "    task_prompt = (\n",
    "        f\"List and briefly summarize the purpose of each file in this directory: {user_input}. \"\n",
    "        \"Ignore executable files and directories. Don't read *.out files. \"\n",
    "        \"Use `list_all_files_recursive` to get all files. Use `read_file` to read each. \"\n",
    "        \"Skip files that are not code, too large, or unreadable.\"\n",
    "    )\n",
    "\n",
    "    # Define agent interaction pattern\n",
    "    agent_pattern = DefaultPattern(\n",
    "        agents=[mcp_agent, joker, user_proxy],\n",
    "        initial_agent=mcp_agent,\n",
    "    )\n",
    "\n",
    "    # Run the task\n",
    "    hello = await a_initiate_group_chat(\n",
    "        pattern=agent_pattern,\n",
    "        messages=[task_prompt],\n",
    "        max_rounds=20,  # Increase if needed\n",
    "    )\n",
    "    \n",
    "\n",
    "# Server params for MCP stdio agent\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"python\",\n",
    "    args=[str(mcp_server_path), \"stdio\"],\n",
    ")\n",
    "\n",
    "import os\n",
    "os.environ[\"AUTOGEN_USE_DOCKER\"] = \"False\"\n",
    "\n",
    "# Launch chat\n",
    "async with stdio_client(server_params) as (read, write), ClientSession(read, write) as session:\n",
    "    await session.initialize()\n",
    "    await create_toolkit_and_run(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130fa32c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
